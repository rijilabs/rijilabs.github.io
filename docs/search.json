[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RiJiLabs",
    "section": "",
    "text": "Random Data Science and Art\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Newest\n        \n         \n          Date - Oldest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nFetching & Preparing Data: stock market data for S&P 500 2\n\n\n\n\nscraping\n\n\nS&P 500\n\n\npoint-in-time\n\n\nsurvivorship-bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nRiJiLabs\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fetching-data-1/index.html#load-company-list",
    "href": "posts/fetching-data-1/index.html#load-company-list",
    "title": "Fetching & Preparing Data: stock market data for S&P 500",
    "section": "1. Load company list",
    "text": "1. Load company list\nOur intention is not to build a process for precise backtesting and/or putting into production any realistic stock trading model. The main reason we picked market data is because it represents a good example of time series, but, given that we are going to use it, we will also emphasize some important points when working with this type of data.\nFirst, we need to decide the set of companies (or universe) that we want to use in our study. In this case, we will work with the 500 largest publicly traded companies in the US, as tracked by the S&P 500 index.\nIt is very important to note that this universe of companies changes over time – constituents of the index can be removed and substituted by another. If we want to do avoid any survivorship bias when looking into the behavior of the universe over time, we should properly cover those changes point-in-time. Since our post has only illustration purposes we are not going to cover this perfectly but we will do what we can by looking into the information available in the S&P 500 Wikipedia page.\nIn order to do that, we will automatically read & load the tables from that page directly in our working environment. There are plenty of ways to scrape the date from a website but here we use already-implemented packages that deal with all the parsing for us:\n\nRPython\n\n\n\nlibrary(data.table)\nlibrary(htmltab)\n\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\ntable1 = data.table(htmltab(url, which = 1))\ntable2 = data.table(htmltab(url, which = 2))\ntable1[1:2, c(1,7)]\n\n   Symbol Date first added\n1:    MMM       1976-08-09\n2:    AOS       2017-07-26\n\ntable2[1:2, c(1,4)]\n\n                Date Removed >> Ticker\n1:     March 2, 2022              INFO\n2: February 15, 2022              XLNX\n\n\n\n\n\nimport requests\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\nre = requests.get(url)\ntables = pd.read_html(re.text)\ntable1 = tables[0]\ntable2 = tables[1]\ntable2.columns = [\"_\".join(a) for a in table2.columns.to_flat_index()] \ntable1.iloc[:2, [0,6]]\n\n  Symbol Date first added\n0    MMM       1976-08-09\n1    AOS       2017-07-26\n\ntable2.iloc[:2, [0,3]]\n\n           Date_Date Removed_Ticker\n0      March 2, 2022           INFO\n1  February 15, 2022           XLNX\n\n\n\n\n\nNote that we have two tables, one with the starting date when the companies where included into the index and the other with the ending date when some of the companies where removed. In order to have a point-in-time coverage of the index we extract both dates and join them into a table/dataframe. Like this we expect to have, for each company, the starting date and end date (if any) when they entered and left the index over time.\n\nRPython\n\n\n\ntable1 = table1[, .(Symbol, `Date first added`)]\nnames(table1) = c(\"TICKER\",\"START_DATE\")\ntable1[ , START_DATE := as.Date(START_DATE)]\ntable1 = table1[!is.na(TICKER)]\n\ntable2 = table2[, .(`Removed >> Ticker`, Date)]\nnames(table2) = c(\"TICKER\", \"END_DATE\")\ntable2[ , END_DATE := as.Date(END_DATE, format = \"%B %d,%Y\")]\ntable2 = table2[!is.na(TICKER)]\n\nsp500DT = merge(table1,table2,by=\"TICKER\",all=TRUE)\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05       <NA>\n2:     AA       <NA> 2016-11-01\n\n\n\n\n\ntable1 = table1[[\"Symbol\", \"Date first added\"]].copy()\ntable1.columns = [\"TICKER\", \"START_DATE\"]\ntable1[\"START_DATE\"] = pd.to_datetime(table1[\"START_DATE\"], errors = 'coerce')\ntable1 = table1[table1[\"TICKER\"].notna()]\n\ntable2 = table2[[\"Removed_Ticker\", \"Date_Date\"]].copy()\ntable2.columns = [\"TICKER\", \"END_DATE\"]\ntable2[\"END_DATE\"] = pd.to_datetime(table2[\"END_DATE\"], errors = 'coerce', format = \"%B %d, %Y\")\ntable2 = table2[table2[\"TICKER\"].notna()]\n\n\nsp500DT = pd.merge(table1, table2, on = \"TICKER\", how = 'outer')\nsp500DT = sp500DT.sort_values(by = 'TICKER')\nsp500DT.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05        NaT\n618     AA        NaT 2016-11-01\n\n\n\n\n\nJust by looking at the both initial rows it becomes obvious that we have instances where END_DATE is not available – we can assume this means that those companies are still in the index; we will populate these with a future time to avoid any issues using the table (e.g. “2111-11-11”). We also have many instances where START_DATE is missing – similarly, we can assume here that these companies have been in the index since the beginning; we can populate this with any date prior to the dates we are interested in (we will just pick the oldest date available in the table).\nUnfortunately, there are few companies with different END_DATEs for which we are missing some information: these are companies that have been removed from the index several times for which we don’t have any of the corresponding START_DATEs. Because they are very few and we don’t intend to have anything accurate from the sources we are using, we will simply take the last end date and assume they where inside the index all the time up to there. To do that we simply get the row with maximum END_DATE per TICKER.\n\nRPython\n\n\n\nsp500DT[is.na(END_DATE), END_DATE := as.Date(\"2111-11-11\")]\nsp500DT[is.na(START_DATE), START_DATE :=min(sp500DT$START_DATE, na.rm = TRUE)]\nsp500DT = sp500DT[ , .SD[which.max(END_DATE)], by = TICKER]\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05 2111-11-11\n2:     AA 1957-03-04 2016-11-01\n\n\n\n\n\nsp500DT[\"END_DATE\"] = sp500DT.END_DATE.fillna(pd.to_datetime(\"2111-11-11\"))\nsp500DT[\"START_DATE\"] = sp500DT.START_DATE.fillna(sp500DT.START_DATE.min())\nsp500DT = sp500DT.sort_values('END_DATE').groupby('TICKER').tail(1).sort_values(by = 'TICKER')\nsp500DT.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05 2111-11-11\n618     AA 1957-03-04 2016-11-01"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Random Data Science and ArtAbout this blog"
  },
  {
    "objectID": "posts/event-studies/index.html",
    "href": "posts/event-studies/index.html",
    "title": "Event Studies",
    "section": "",
    "text": "Random Data Science and Art"
  },
  {
    "objectID": "posts/fetching-data-1/index.html#load-sp-500-constituents",
    "href": "posts/fetching-data-1/index.html#load-sp-500-constituents",
    "title": "Fetching & Preparing Data: stock market data for S&P 500",
    "section": "1. Load S&P 500 constituents",
    "text": "1. Load S&P 500 constituents\nOur intention is not to build a process for precise backtesting and/or putting into production any realistic stock trading model. The main reason we picked market data is because it represents a good example of time series, but, given that we are going to use it, we will also emphasize some important points when working with this type of data.\nFirst, we need to decide the set of companies (or universe) that we want to use in our study. In this case, we will work with the 500 largest publicly traded companies in the US, as tracked by the S&P 500 index.\n\n\n\n\n\n\nCaution\n\n\n\nIt is very important to note that this universe of companies changes over time – constituents of the index can be removed and substituted by anothers. If we want to do avoid any survivorship bias when looking into the behavior of the universe over time, we should properly cover those changes point-in-time.\n\n\nSince our post has only illustration purposes we are not going to cover this perfectly but we will do what we can by looking into the information available in the S&P 500 Wikipedia page.\nIn order to do that, we will automatically read & load the tables from that page directly into our working environment. There are plenty of ways to scrape data from a website, here we use already-implemented packages that deal with all the parsing for us:\n\nRPython\n\n\n\nlibrary(data.table)\nlibrary(htmltab)\n\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\ntable1 = data.table(htmltab(url, which = 1))\ntable2 = data.table(htmltab(url, which = 2))\ntable1[1:2, c(1,7)]\n\n   Symbol Date first added\n1:    MMM       1976-08-09\n2:    AOS       2017-07-26\n\ntable2[1:2, c(1,4)]\n\n                Date Removed >> Ticker\n1:     March 2, 2022              INFO\n2: February 15, 2022              XLNX\n\n\n\n\n\nimport requests\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\nre = requests.get(url)\ntables = pd.read_html(re.text)\ntable1 = tables[0]\ntable2 = tables[1]\ntable2.columns = [\"_\".join(a) for a in table2.columns.to_flat_index()] \ntable1.iloc[:2, [0,6]]\n\n  Symbol Date first added\n0    MMM       1976-08-09\n1    AOS       2017-07-26\n\ntable2.iloc[:2, [0,3]]\n\n           Date_Date Removed_Ticker\n0      March 2, 2022           INFO\n1  February 15, 2022           XLNX\n\n\n\n\n\nNote that we have two tables, one with the starting date when the companies where included into the index and the other with the ending date when some of the companies where removed. In order to have a point-in-time coverage of the index we extract both dates and join them into a table/dataframe. Like this we expect to have, for each company, the starting date and end date (if any) when they entered and left the index over time.\n\nRPython\n\n\n\ntable1 = table1[, .(Symbol, `Date first added`)]\nnames(table1) = c(\"TICKER\",\"START_DATE\")\ntable1[ , START_DATE := as.Date(START_DATE)]\ntable1 = table1[!is.na(TICKER)]\n\ntable2 = table2[, .(`Removed >> Ticker`, Date)]\nnames(table2) = c(\"TICKER\", \"END_DATE\")\ntable2[ , END_DATE := as.Date(END_DATE, format = \"%B %d,%Y\")]\ntable2 = table2[!is.na(TICKER)]\n\nsp500DT = merge(table1,table2,by=\"TICKER\",all=TRUE)\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05       <NA>\n2:     AA       <NA> 2016-11-01\n\n\n\n\n\ntable1 = table1[[\"Symbol\", \"Date first added\"]].copy()\ntable1.columns = [\"TICKER\", \"START_DATE\"]\ntable1[\"START_DATE\"] = pd.to_datetime(table1[\"START_DATE\"], errors = 'coerce')\ntable1 = table1[table1[\"TICKER\"].notna()]\n\ntable2 = table2[[\"Removed_Ticker\", \"Date_Date\"]].copy()\ntable2.columns = [\"TICKER\", \"END_DATE\"]\ntable2[\"END_DATE\"] = pd.to_datetime(table2[\"END_DATE\"], errors = 'coerce', format = \"%B %d, %Y\")\ntable2 = table2[table2[\"TICKER\"].notna()]\n\n\nsp500DT = pd.merge(table1, table2, on = \"TICKER\", how = 'outer')\nsp500DT = sp500DT.sort_values(by = 'TICKER')\nsp500DT.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05        NaT\n618     AA        NaT 2016-11-01\n\n\n\n\n\nJust by looking at the both initial rows it becomes obvious that we have instances where END_DATE is not available – we can assume this means that those companies are still constituents of the index as of today; we will populate these with a future time to avoid any issues when using the table (e.g. “2111-11-11”). We also have many instances where START_DATE is missing – similarly, we can assume here that these companies have been in the index since the beginning; we can populate this with any date prior to the dates we are interested in (we will just pick the oldest date available in the table).\nUnfortunately, there are few companies with different END_DATEs for which we are missing some information: these are companies that have been incorporated and removed from the index several times, for which we don’t have any of the corresponding START_DATEs. Because there are very few cases and we don’t intend to have very accurate results anyway, we will simply take the last END_DATE and assume these companies were inside the index all the time up until then. To do that we simply get the row with maximum END_DATE per TICKER.\n\nRPython\n\n\n\nsp500DT[is.na(END_DATE), END_DATE := as.Date(\"2111-11-11\")]\nsp500DT[is.na(START_DATE), START_DATE :=min(sp500DT$START_DATE, na.rm = TRUE)]\nsp500DT = sp500DT[ , .SD[which.max(END_DATE)], by = TICKER]\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05 2111-11-11\n2:     AA 1957-03-04 2016-11-01\n\n\n\n\n\nsp500DT[\"END_DATE\"] = sp500DT.END_DATE.fillna(pd.to_datetime(\"2111-11-11\"))\nsp500DT[\"START_DATE\"] = sp500DT.START_DATE.fillna(sp500DT.START_DATE.min())\nsp500DT = sp500DT.sort_values('END_DATE').groupby('TICKER').tail(1).sort_values(by = 'TICKER')\nsp500DT.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05 2111-11-11\n618     AA 1957-03-04 2016-11-01"
  },
  {
    "objectID": "posts/fetching-data-1/index.html#get-market-data",
    "href": "posts/fetching-data-1/index.html#get-market-data",
    "title": "Fetching & Preparing Data: stock market data for S&P 500",
    "section": "2. Get market data",
    "text": "2. Get market data\n\n\n\n\n\n\nNote\n\n\n\nvfd\n\n\nAt this point we have a point-in-time version of the S&P 500 index, with its different constituents over time."
  },
  {
    "objectID": "posts/fetching-data-1/index.html#get-stock-market-data",
    "href": "posts/fetching-data-1/index.html#get-stock-market-data",
    "title": "Fetching & Preparing Data: stock market data for S&P 500",
    "section": "2. Get stock market data",
    "text": "2. Get stock market data\nNow that we have a point-in-time version of the S&P 500 index, with all its constituents over time, we can move to retrieving the market data."
  },
  {
    "objectID": "posts/fetching-data-2/index.html#load-sp-500-constituents",
    "href": "posts/fetching-data-2/index.html#load-sp-500-constituents",
    "title": "Fetching & Preparing Data: stock market data for S&P 500 2",
    "section": "1. Load S&P 500 constituents",
    "text": "1. Load S&P 500 constituents\nOur intention is not to build a process for precise backtesting and/or putting into production any realistic stock trading model. The main reason we picked market data is because it represents a good example of time series, but, given that we are going to use it, we will also emphasize some important points when working with this type of data.\nFirst, we need to decide the set of companies (or universe) that we want to use in our study. In this case, we will work with the 500 largest publicly traded companies in the US, as tracked by the S&P 500 index.\n\n\n\n\n\n\nCaution\n\n\n\nIt is very important to note that this universe of companies changes over time – constituents of the index can be removed and substituted by anothers. If we want to do avoid any survivorship bias when looking into the behavior of the universe over time, we should properly cover those changes point-in-time.\n\n\nSince our post has only illustration purposes we are not going to cover this perfectly but we will do what we can by looking into the information available in the S&P 500 Wikipedia page.\nIn order to do that, we will automatically read & load the tables from that page directly into our working environment. There are plenty of ways to scrape data from a website, here we use already-implemented packages that deal with all the parsing for us:\n\nRPython\n\n\n\nlibrary(data.table)\nlibrary(htmltab)\n\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\ntable1 = data.table(htmltab(url, which = 1))\ntable2 = data.table(htmltab(url, which = 2))\ntable1[1:2, c(1,7)]\n\n   Symbol Date first added\n1:    MMM       1976-08-09\n2:    AOS       2017-07-26\n\ntable2[1:2, c(1,4)]\n\n                Date Removed >> Ticker\n1:     March 2, 2022              INFO\n2: February 15, 2022              XLNX\n\n\n\n\n\nimport requests\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\nre = requests.get(url)\ntables = pd.read_html(re.text)\ntable1 = tables[0]\ntable2 = tables[1]\ntable2.columns = [\"_\".join(a) for a in table2.columns.to_flat_index()] \ntable1.iloc[:2, [0,6]]\n\n  Symbol Date first added\n0    MMM       1976-08-09\n1    AOS       2017-07-26\n\ntable2.iloc[:2, [0,3]]\n\n           Date_Date Removed_Ticker\n0      March 2, 2022           INFO\n1  February 15, 2022           XLNX\n\n\n\n\n\nNote that we have two tables, one with the starting date when the companies where included into the index and the other with the ending date when some of the companies where removed. In order to have a point-in-time coverage of the index we extract both dates and join them into a table/dataframe. Like this we expect to have, for each company, the starting date and end date (if any) when they entered and left the index over time.\n\nRPython\n\n\n\ntable1 = table1[, .(Symbol, `Date first added`)]\nnames(table1) = c(\"TICKER\",\"START_DATE\")\ntable1[ , START_DATE := as.Date(START_DATE)]\ntable1 = table1[!is.na(TICKER)]\n\ntable2 = table2[, .(`Removed >> Ticker`, Date)]\nnames(table2) = c(\"TICKER\", \"END_DATE\")\ntable2[ , END_DATE := as.Date(END_DATE, format = \"%B %d,%Y\")]\ntable2 = table2[!is.na(TICKER)]\n\nsp500DT = merge(table1,table2,by=\"TICKER\",all=TRUE)\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05       <NA>\n2:     AA       <NA> 2016-11-01\n\n\n\n\n\ntable1 = table1[[\"Symbol\", \"Date first added\"]].copy()\ntable1.columns = [\"TICKER\", \"START_DATE\"]\ntable1[\"START_DATE\"] = pd.to_datetime(table1[\"START_DATE\"], errors = 'coerce')\ntable1 = table1[table1[\"TICKER\"].notna()]\n\ntable2 = table2[[\"Removed_Ticker\", \"Date_Date\"]].copy()\ntable2.columns = [\"TICKER\", \"END_DATE\"]\ntable2[\"END_DATE\"] = pd.to_datetime(table2[\"END_DATE\"], errors = 'coerce', format = \"%B %d, %Y\")\ntable2 = table2[table2[\"TICKER\"].notna()]\n\n\nsp500DT = pd.merge(table1, table2, on = \"TICKER\", how = 'outer')\nsp500DT = sp500DT.sort_values(by = 'TICKER')\nsp500DT.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05        NaT\n618     AA        NaT 2016-11-01\n\n\n\n\n\nJust by looking at the both initial rows it becomes obvious that we have instances where END_DATE is not available – we can assume this means that those companies are still constituents of the index as of today; we will populate these with a future time to avoid any issues when using the table (e.g. “2111-11-11”). We also have many instances where START_DATE is missing – similarly, we can assume here that these companies have been in the index since the beginning; we can populate this with any date prior to the dates we are interested in (we will just pick the oldest date available in the table).\nUnfortunately, there are few companies with different END_DATEs for which we are missing some information: these are companies that have been incorporated and removed from the index several times, for which we don’t have any of the corresponding START_DATEs. Because there are very few cases and we don’t intend to have very accurate results anyway, we will simply take the last END_DATE and assume these companies were inside the index all the time up until then. To do that we simply get the row with maximum END_DATE per TICKER.\n\nRPython\n\n\n\nsp500DT[is.na(END_DATE), END_DATE := as.Date(\"2111-11-11\")]\nsp500DT[is.na(START_DATE), START_DATE :=min(sp500DT$START_DATE, na.rm = TRUE)]\nsp500DT = sp500DT[ , .SD[which.max(END_DATE)], by = TICKER]\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05 2111-11-11\n2:     AA 1957-03-04 2016-11-01\n\n\n\n\n\nsp500DT[\"END_DATE\"] = sp500DT.END_DATE.fillna(pd.to_datetime(\"2111-11-11\"))\nsp500DT[\"START_DATE\"] = sp500DT.START_DATE.fillna(sp500DT.START_DATE.min())\nsp500DT = sp500DT.sort_values('END_DATE').groupby('TICKER').tail(1).sort_values(by = 'TICKER')\nsp500DT.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05 2111-11-11\n618     AA 1957-03-04 2016-11-01"
  },
  {
    "objectID": "posts/fetching-data-2/index.html#get-stock-market-data",
    "href": "posts/fetching-data-2/index.html#get-stock-market-data",
    "title": "Fetching & Preparing Data: stock market data for S&P 500 2",
    "section": "2. Get stock market data",
    "text": "2. Get stock market data\nNow that we have a point-in-time version of the S&P 500 index, with all its constituents over time, we can move to retrieving the market data."
  }
]