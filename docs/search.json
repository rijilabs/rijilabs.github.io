[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RiJiLabs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Newest\n        \n         \n          Date - Oldest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nFetching & Preparing Data: stock market data for S&P 500\n\n\n\n\nscraping\n\n\nS&P 500\n\n\npoint-in-time\n\n\nsurvivorship-bias\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nRiJiLabs\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fetching-data-1/index.html#load-company-list",
    "href": "posts/fetching-data-1/index.html#load-company-list",
    "title": "Fetching & Preparing Data: stock market data for S&P 500",
    "section": "1. Load company list",
    "text": "1. Load company list\nOur intention is not to build a process for precise backtesting and/or putting into production any realistic stock trading model. The main reason we picked market data is because it represents a good example of time series, but, given that we are going to use it, we will also emphasize some important points when working with this type of data.\nFirst, we need to decide the set of companies (or universe) that we want to use in our study. In this case, we will work with the 500 largest publicly traded companies in the US, as tracked by the S&P 500 index.\nIt is very important to note that this universe of companies changes over time â€“ constituents of the index can be removed and substituted by another. If we want to do avoid any survivorship bias when looking into the behavior of the universe over time, we should properly cover those changes point-in-time. Since our post has only illustration purposes we are not going to cover this perfectly but we will do what we can by looking into the information available in the S&P 500 Wikipedia page.\nIn order to do that, we will automatically read & load the tables from that page directly in our working environment. There are plenty of ways to scrape the date from a website but here we use already-implemented packages that deal with all the parsing for us:\n\nRPython\n\n\n\nlibrary(data.table)\nlibrary(htmltab)\n\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\ntable1 = data.table(htmltab(url,which = 1))\ntable2 = data.table(htmltab(url,which = 2))\ntable1[1:2,c(1,7)]\n\n   Symbol Date first added\n1:    MMM       1976-08-09\n2:    AOS       2017-07-26\n\ntable2[1:2,c(1,4)]\n\n                Date Removed >> Ticker\n1:     March 2, 2022              INFO\n2: February 15, 2022              XLNX\n\n\n\n\n\nimport requests\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\nre = requests.get(url)\ntables = pd.read_html(re.text)\ntable1 = tables[0]\ntable2 = tables[1]\ntable2.columns = [\"_\".join(a) for a in table2.columns.to_flat_index()] \ntable1.iloc[:2,[0,6]]\n\n  Symbol Date first added\n0    MMM       1976-08-09\n1    AOS       2017-07-26\n\ntable2.iloc[:2,[0,3]]\n\n           Date_Date Removed_Ticker\n0      March 2, 2022           INFO\n1  February 15, 2022           XLNX\n\n\n\n\n\nNote that we have two tables, one with the starting date when the companies where included into the index and the other with the ending date when some of the companies where removed. In order to have a point-in-time coverage of the index we extract both dates and join them into a table/dataframe.\n\nRPython\n\n\n\ntable1 = table1[,.(Symbol, `Date first added`)]\nnames(table1) = c(\"TICKER\",\"START_DATE\")\ntable1[, START_DATE := as.Date(START_DATE)]\ntable1 = table1[!is.na(TICKER)]\n\ntable2 = table2[,.(`Removed >> Ticker`, Date)]\nnames(table2) = c(\"TICKER\", \"END_DATE\")\ntable2[, END_DATE := as.Date(END_DATE, format=\"%B %d,%Y\")]\ntable2 = table2[!is.na(TICKER)]\n\nsp500DT = merge(table1,table2,by=\"TICKER\",all=TRUE)\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05       <NA>\n2:     AA       <NA> 2016-11-01\n\n\n\n\n\ntable1=table1[[\"Symbol\",\"Date first added\"]].copy()\ntable1.columns = [\"TICKER\",\"START_DATE\"]\ntable1[\"START_DATE\"] = pd.to_datetime(table1[\"START_DATE\"], errors = 'coerce')\ntable1 = table1[table1[\"TICKER\"].notna()]\n\ntable2 = table2[[\"Removed_Ticker\",\"Date_Date\"]].copy()\ntable2.columns = [\"TICKER\",\"END_DATE\"]\ntable2[\"END_DATE\"] = pd.to_datetime(table2[\"END_DATE\"], errors = 'coerce', format = \"%B %d, %Y\")\ntable2 = table2[table2[\"TICKER\"].notna()]\n\n\nsp500DT = pd.merge(table1, table2, on = \"TICKER\", how = 'outer')\nsp500DT = sp500DT.sort_values(by='TICKER')\nsp500DT.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05        NaT\n618     AA        NaT 2016-11-01"
  }
]