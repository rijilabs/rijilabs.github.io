[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RiJiLabs",
    "section": "",
    "text": "Random Data Science and Art\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Newest\n        \n         \n          Date - Oldest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nEvent Studies\n\n\n\nevent studies\n\n\nmarket impact\n\n\n\ncoming soon\n\n\n\nRiJiLabs\n\n\n30/03/2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFetching & Preparing Data\n\n\n\nscraping\n\n\nS&P 500\n\n\npoint-in-time\n\n\nsurvivorship-bias\n\n\n\nStock market data for S&P 500\n\n\n\nRiJiLabs\n\n\n25/03/2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fetching-data-1/index.html#load-company-list",
    "href": "posts/fetching-data-1/index.html#load-company-list",
    "title": "Fetching & Preparing Data: stock market data for S&P 500",
    "section": "1. Load company list",
    "text": "1. Load company list\nOur intention is not to build a process for precise backtesting and/or putting into production any realistic stock trading model. The main reason we picked market data is because it represents a good example of time series, but, given that we are going to use it, we will also emphasize some important points when working with this type of data.\nFirst, we need to decide the set of companies (or universe) that we want to use in our study. In this case, we will work with the 500 largest publicly traded companies in the US, as tracked by the S&P 500 index.\nIt is very important to note that this universe of companies changes over time – constituents of the index can be removed and substituted by another. If we want to do avoid any survivorship bias when looking into the behavior of the universe over time, we should properly cover those changes point-in-time. Since our post has only illustration purposes we are not going to cover this perfectly but we will do what we can by looking into the information available in the S&P 500 Wikipedia page.\nIn order to do that, we will automatically read & load the tables from that page directly in our working environment. There are plenty of ways to scrape the date from a website but here we use already-implemented packages that deal with all the parsing for us:\n\nRPython\n\n\n\nlibrary(data.table)\nlibrary(htmltab)\n\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\ntable1 = data.table(htmltab(url, which = 1))\ntable2 = data.table(htmltab(url, which = 2))\ntable1[1:2, c(1,7)]\n\n   Symbol Date first added\n1:    MMM       1976-08-09\n2:    AOS       2017-07-26\n\ntable2[1:2, c(1,4)]\n\n                Date Removed >> Ticker\n1:     March 2, 2022              INFO\n2: February 15, 2022              XLNX\n\n\n\n\n\nimport requests\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\nre = requests.get(url)\ntables = pd.read_html(re.text)\ntable1 = tables[0]\ntable2 = tables[1]\ntable2.columns = [\"_\".join(a) for a in table2.columns.to_flat_index()] \ntable1.iloc[:2, [0,6]]\n\n  Symbol Date first added\n0    MMM       1976-08-09\n1    AOS       2017-07-26\n\ntable2.iloc[:2, [0,3]]\n\n           Date_Date Removed_Ticker\n0      March 2, 2022           INFO\n1  February 15, 2022           XLNX\n\n\n\n\n\nNote that we have two tables, one with the starting date when the companies where included into the index and the other with the ending date when some of the companies where removed. In order to have a point-in-time coverage of the index we extract both dates and join them into a table/dataframe. Like this we expect to have, for each company, the starting date and end date (if any) when they entered and left the index over time.\n\nRPython\n\n\n\ntable1 = table1[, .(Symbol, `Date first added`)]\nnames(table1) = c(\"TICKER\",\"START_DATE\")\ntable1[ , START_DATE := as.Date(START_DATE)]\ntable1 = table1[!is.na(TICKER)]\n\ntable2 = table2[, .(`Removed >> Ticker`, Date)]\nnames(table2) = c(\"TICKER\", \"END_DATE\")\ntable2[ , END_DATE := as.Date(END_DATE, format = \"%B %d,%Y\")]\ntable2 = table2[!is.na(TICKER)]\n\nsp500DT = merge(table1,table2,by=\"TICKER\",all=TRUE)\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05       <NA>\n2:     AA       <NA> 2016-11-01\n\n\n\n\n\ntable1 = table1[[\"Symbol\", \"Date first added\"]].copy()\ntable1.columns = [\"TICKER\", \"START_DATE\"]\ntable1[\"START_DATE\"] = pd.to_datetime(table1[\"START_DATE\"], errors = 'coerce')\ntable1 = table1[table1[\"TICKER\"].notna()]\n\ntable2 = table2[[\"Removed_Ticker\", \"Date_Date\"]].copy()\ntable2.columns = [\"TICKER\", \"END_DATE\"]\ntable2[\"END_DATE\"] = pd.to_datetime(table2[\"END_DATE\"], errors = 'coerce', format = \"%B %d, %Y\")\ntable2 = table2[table2[\"TICKER\"].notna()]\n\n\nsp500DT = pd.merge(table1, table2, on = \"TICKER\", how = 'outer')\nsp500DT = sp500DT.sort_values(by = 'TICKER')\nsp500DT.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05        NaT\n618     AA        NaT 2016-11-01\n\n\n\n\n\nJust by looking at the both initial rows it becomes obvious that we have instances where END_DATE is not available – we can assume this means that those companies are still in the index; we will populate these with a future time to avoid any issues using the table (e.g. “2111-11-11”). We also have many instances where START_DATE is missing – similarly, we can assume here that these companies have been in the index since the beginning; we can populate this with any date prior to the dates we are interested in (we will just pick the oldest date available in the table).\nUnfortunately, there are few companies with different END_DATEs for which we are missing some information: these are companies that have been removed from the index several times for which we don’t have any of the corresponding START_DATEs. Because they are very few and we don’t intend to have anything accurate from the sources we are using, we will simply take the last end date and assume they where inside the index all the time up to there. To do that we simply get the row with maximum END_DATE per TICKER.\n\nRPython\n\n\n\nsp500DT[is.na(END_DATE), END_DATE := as.Date(\"2111-11-11\")]\nsp500DT[is.na(START_DATE), START_DATE :=min(sp500DT$START_DATE, na.rm = TRUE)]\nsp500DT = sp500DT[ , .SD[which.max(END_DATE)], by = TICKER]\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05 2111-11-11\n2:     AA 1957-03-04 2016-11-01\n\n\n\n\n\nsp500DT[\"END_DATE\"] = sp500DT.END_DATE.fillna(pd.to_datetime(\"2111-11-11\"))\nsp500DT[\"START_DATE\"] = sp500DT.START_DATE.fillna(sp500DT.START_DATE.min())\nsp500DT = sp500DT.sort_values('END_DATE').groupby('TICKER').tail(1).sort_values(by = 'TICKER')\nsp500DT.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05 2111-11-11\n618     AA 1957-03-04 2016-11-01"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Random Data Science and ArtAbout this blog"
  },
  {
    "objectID": "posts/event-studies/index.html",
    "href": "posts/event-studies/index.html",
    "title": "Event Studies",
    "section": "",
    "text": "Random Data Science and Art"
  },
  {
    "objectID": "posts/fetching-data-1/index.html#load-sp-500-constituents",
    "href": "posts/fetching-data-1/index.html#load-sp-500-constituents",
    "title": "Fetching & Preparing Data",
    "section": "1. Load S&P 500 constituents",
    "text": "1. Load S&P 500 constituents\nOur intention is not to build a process for precise backtesting and/or putting into production any realistic stock trading model. The main reason we picked market data is because it represents a good example of time series, but, given that we are going to use it, we will also emphasize some important points when working with this type of data.\nFirst, we need to decide the set of companies (or universe) that we want to use in our study. In this case, we will work with the 500 largest publicly traded companies in the US, as tracked by the S&P 500 index.\n\n\n\n\n\n\nCaution\n\n\n\nIt is very important to note that this universe of companies changes over time – constituents of the index can be removed and substituted by others. If we want to do avoid any survivorship bias when looking into the behavior of the universe over time, we should properly cover those changes point-in-time.\n\n\nSince our post has only illustration purposes we are not going to cover this perfectly but we will do what we can by looking into the information available in the S&P 500 Wikipedia page.\nIn order to do that, we will automatically read & load the tables from that page directly into our working environment. There are plenty of ways to scrape data from a website, here we use already-implemented packages that deal with all the parsing for us:\n\nRPython\n\n\n\nlibrary(data.table)\nlibrary(htmltab)\n\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\ntable1 = data.table(htmltab(url, which = 1))\ntable2 = data.table(htmltab(url, which = 2))\ntable1[1:2, c(1,7)]\n\n   Symbol Date first added\n1:    MMM       1976-08-09\n2:    AOS       2017-07-26\n\ntable2[1:2, c(1,4)]\n\n                Date Removed >> Ticker\n1:     March 2, 2022              INFO\n2: February 15, 2022              XLNX\n\n\n\n\n\nimport requests\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\nre = requests.get(url)\ntables = pd.read_html(re.text)\ntable1 = tables[0]\ntable2 = tables[1]\ntable2.columns = [\"_\".join(a) for a in table2.columns.to_flat_index()] \ntable1.iloc[:2, [0,6]]\n\n  Symbol Date first added\n0    MMM       1976-08-09\n1    AOS       2017-07-26\n\ntable2.iloc[:2, [0,3]]\n\n           Date_Date Removed_Ticker\n0      March 2, 2022           INFO\n1  February 15, 2022           XLNX\n\n\n\n\n\nNote that we have two tables, one with the starting date when the companies where included into the index and the other with the ending date when some of the companies where removed. In order to have a point-in-time coverage of the index we extract both dates and join them into a table/dataframe. Like this we expect to have, for each company, the starting date and end date (if any) when they entered and left the index over time.\n\n\n\n\n\n\nNote\n\n\n\nWe are going to use the tickers (symbols) as the company identifier, since this is all we have, other than the name, from the Wiki page. This is not a very good practice since tickers can change over time and can be reused for different companies, but we will ignore any potential issues here.\n\n\n\nRPython\n\n\n\ntable1 = table1[, .(Symbol, `Date first added`)]\nnames(table1) = c(\"TICKER\", \"START_DATE\")\ntable1[ , START_DATE := as.Date(START_DATE)]\ntable1 = table1[!is.na(TICKER)]\n\ntable2 = table2[, .(`Removed >> Ticker`, Date)]\nnames(table2) = c(\"TICKER\", \"END_DATE\")\ntable2[ , END_DATE := as.Date(END_DATE, format = \"%B %d,%Y\")]\ntable2 = table2[!is.na(TICKER)]\n\nsp500DT = merge(table1,table2,by=\"TICKER\",all=TRUE)\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05       <NA>\n2:     AA       <NA> 2016-11-01\n\n\n\n\n\ntable1 = table1[[\"Symbol\", \"Date first added\"]].copy()\ntable1.columns = [\"TICKER\", \"START_DATE\"]\ntable1[\"START_DATE\"] = pd.to_datetime(table1[\"START_DATE\"], errors = 'coerce')\ntable1 = table1[table1[\"TICKER\"].notna()]\n\ntable2 = table2[[\"Removed_Ticker\", \"Date_Date\"]].copy()\ntable2.columns = [\"TICKER\", \"END_DATE\"]\ntable2[\"END_DATE\"] = pd.to_datetime(table2[\"END_DATE\"], errors = 'coerce', format = \"%B %d, %Y\")\ntable2 = table2[table2[\"TICKER\"].notna()]\n\n\nsp500DF = pd.merge(table1, table2, on = \"TICKER\", how = 'outer')\nsp500DF = sp500DF.sort_values(by = 'TICKER')\nsp500DF.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05        NaT\n618     AA        NaT 2016-11-01\n\n\n\n\n\nJust by looking at the both initial rows it becomes obvious that we have instances where END_DATE is not available – we can assume this means that those companies are still constituents of the index as of today; we will populate these with a future time to avoid any issues when using the table (e.g. “2111-11-11”). We also have many instances where START_DATE is missing – similarly, we can assume here that these companies have been in the index since the beginning; we can populate this with any date prior to the dates we are interested in (we will just pick the oldest date available in the table).\nUnfortunately, there are few tickers with multiple END_DATEs: these could represent companies that have been incorporated and removed from the index several times, or, as we discussed earlier, mean that a ticker is being reused for a different company. Either case, we are missing some information: we don’t have any of the corresponding START_DATEs. Because there are very few cases and we don’t intend to have very accurate results anyway, we will simply take the last END_DATE and assume this ticker represents a company that was inside the index up until that date. In order to do that we simply get the row with maximum END_DATE per TICKER.\n\nRPython\n\n\n\nsp500DT[is.na(END_DATE), END_DATE := as.Date(\"2111-11-11\")]\nsp500DT[is.na(START_DATE), START_DATE :=min(sp500DT$START_DATE, na.rm = TRUE)]\nsp500DT = sp500DT[ , .SD[which.max(END_DATE)], by = TICKER]\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05 2111-11-11\n2:     AA 1957-03-04 2016-11-01\n\n\n\n\n\nsp500DF[\"END_DATE\"] = sp500DF.END_DATE.fillna(pd.to_datetime(\"2111-11-11\"))\nsp500DF[\"START_DATE\"] = sp500DF.START_DATE.fillna(sp500DF.START_DATE.min())\nsp500DF = sp500DF.sort_values('END_DATE').groupby('TICKER').tail(1).sort_values(by = 'TICKER')\nsp500DF.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05 2111-11-11\n618     AA 1957-03-04 2016-11-01"
  },
  {
    "objectID": "posts/fetching-data-1/index.html#get-market-data",
    "href": "posts/fetching-data-1/index.html#get-market-data",
    "title": "Fetching & Preparing Data: stock market data for S&P 500",
    "section": "2. Get market data",
    "text": "2. Get market data\n\n\n\n\n\n\nNote\n\n\n\nvfd\n\n\nAt this point we have a point-in-time version of the S&P 500 index, with its different constituents over time."
  },
  {
    "objectID": "posts/fetching-data-1/index.html#get-stock-market-data",
    "href": "posts/fetching-data-1/index.html#get-stock-market-data",
    "title": "Fetching & Preparing Data",
    "section": "2. Get stock market data",
    "text": "2. Get stock market data\nNow that we have a point-in-time version of the S&P 500 index, with all its constituents over time, we can move to retrieving the market data. We will use Yahoo! Finance and its internal API to request historical stock market data.\n\n\n\n\n\n\nNote\n\n\n\nThe format of the queries to request data may change in the future. In order to figure this out, you can navigate into the website to manually request some data and check the URL that appears in the Download button by Inspecting the website/entering the Web Developer Tools. For example, for this, the corresponding Request URL at the moment of writting this post is https://query1.finance.yahoo.com/v7/finance/download/AAPL?period1=1640476800&period2=1648252800&int\n\n\nIn particular, we will get the default daily pricing and trading volume data from “2000-01-01” to “2022-03-01” for all the companies in the index (if available). We have to request data company by company so we loop over all available tickers:\n\nRPython\n\n\n\nstartDate = as.POSIXct(\"2000-01-01\",tz = \"GMT\")\nendDate = as.POSIXct(\"2022-03-01\",tz = \"GMT\")\n\nmarketDT = NULL\nfor(ticker in sp500DT$TICKER){ \n  \n  url2 = paste0(\"https://query1.finance.yahoo.com/v7/finance/download/\",\n               ticker,\n               \"?period1=\", as.integer(startDate),\n               \"&period2=\", as.integer(endDate),\n               \"&interval=1d&events=history\"\n               )\n  marketDT0 = tryCatch(\n    {read.csv(url2)},\n    error = function(e){return(NULL)},\n    warning = function(w){}\n  )\n  \n  if(!is.null(marketDT0)){\n    setDT(marketDT0)\n    marketDT0[ , TICKER := ticker]\n    marketDT = rbind(marketDT, marketDT0)\n    rm(marketDT0)\n  }\n  Sys.sleep(5) # to avoid issues with request limits we slow down the process\n}\nnames(marketDT) = toupper(names(marketDT))\nsetkey(marketDT, DATE, TICKER)\n\n\nmarketDT[1:2]\n\n         DATE      OPEN       HIGH       LOW     CLOSE ADJ.CLOSE  VOLUME TICKER\n1: 2000-01-03 56.330471  56.464592 48.193848 51.502148 43.812588 4674353      A\n2: 2000-01-03 99.724503 100.400345 96.570564 97.246407 71.984177 1291386     AA\n\n\n\n\n\nimport time\n\nstartDate = pd.to_datetime(\"01-01-2000\"))\nendDate = pd.to_datetime(\"03-01-2022\"))\n\nmarketDF = pd.DataFrame()\nfor ticker in sp500DF.TICKER:\n  url2 = (\"https://query1.finance.yahoo.com/v7/finance/download/\"\n  + ticker\n  + \"?period1=\" +   str(int(startDate.value / 1000000000)) \n  + \"&period2=\" + str(int(endDate.value / 1000000000)) \n  + \"&interval=1d&events=history\")\n  \n  try: \n    marketDF0 = pd.read_csv(url2)\n  except:\n    pass\n  else: \n    marketDF0['TICKER'] = ticker \n    marketDF = marketDF.append(marketDF0)\n  \n  time.sleep(5) # to avoid issues with request limits we slow down the process\n\nmarketDF.columns=marketDF.columns.str.upper()\nmarketDF = marketDF.sort_values(by = ['DATE', 'TICKER'])\n\n\nmarketDF.iloc[:2]\n\n              DATE       OPEN        HIGH  ...  ADJ CLOSE     VOLUME  TICKER\n972402  2000-01-03  56.330471   56.464592  ...  43.812580  4674353.0       A\n358542  2000-01-03  99.724503  100.400345  ...  71.984161  1291386.0      AA\n\n[2 rows x 8 columns]"
  },
  {
    "objectID": "posts/fetching-data-2/index.html#load-sp-500-constituents",
    "href": "posts/fetching-data-2/index.html#load-sp-500-constituents",
    "title": "Fetching & Preparing Data: stock market data for S&P 500 2",
    "section": "1. Load S&P 500 constituents",
    "text": "1. Load S&P 500 constituents\nOur intention is not to build a process for precise backtesting and/or putting into production any realistic stock trading model. The main reason we picked market data is because it represents a good example of time series, but, given that we are going to use it, we will also emphasize some important points when working with this type of data.\nFirst, we need to decide the set of companies (or universe) that we want to use in our study. In this case, we will work with the 500 largest publicly traded companies in the US, as tracked by the S&P 500 index.\n\n\n\n\n\n\nCaution\n\n\n\nIt is very important to note that this universe of companies changes over time – constituents of the index can be removed and substituted by anothers. If we want to do avoid any survivorship bias when looking into the behavior of the universe over time, we should properly cover those changes point-in-time.\n\n\nSince our post has only illustration purposes we are not going to cover this perfectly but we will do what we can by looking into the information available in the S&P 500 Wikipedia page.\nIn order to do that, we will automatically read & load the tables from that page directly into our working environment. There are plenty of ways to scrape data from a website, here we use already-implemented packages that deal with all the parsing for us:\n\nRPython\n\n\n\nlibrary(data.table)\nlibrary(htmltab)\n\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\ntable1 = data.table(htmltab(url, which = 1))\ntable2 = data.table(htmltab(url, which = 2))\ntable1[1:2, c(1,7)]\n\n   Symbol Date first added\n1:    MMM       1976-08-09\n2:    AOS       2017-07-26\n\ntable2[1:2, c(1,4)]\n\n                Date Removed >> Ticker\n1:     March 2, 2022              INFO\n2: February 15, 2022              XLNX\n\n\n\n\n\nimport requests\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\nre = requests.get(url)\ntables = pd.read_html(re.text)\ntable1 = tables[0]\ntable2 = tables[1]\ntable2.columns = [\"_\".join(a) for a in table2.columns.to_flat_index()] \ntable1.iloc[:2, [0,6]]\n\n  Symbol Date first added\n0    MMM       1976-08-09\n1    AOS       2017-07-26\n\ntable2.iloc[:2, [0,3]]\n\n           Date_Date Removed_Ticker\n0      March 2, 2022           INFO\n1  February 15, 2022           XLNX\n\n\n\n\n\nNote that we have two tables, one with the starting date when the companies where included into the index and the other with the ending date when some of the companies where removed. In order to have a point-in-time coverage of the index we extract both dates and join them into a table/dataframe. Like this we expect to have, for each company, the starting date and end date (if any) when they entered and left the index over time.\n\nRPython\n\n\n\ntable1 = table1[, .(Symbol, `Date first added`)]\nnames(table1) = c(\"TICKER\",\"START_DATE\")\ntable1[ , START_DATE := as.Date(START_DATE)]\ntable1 = table1[!is.na(TICKER)]\n\ntable2 = table2[, .(`Removed >> Ticker`, Date)]\nnames(table2) = c(\"TICKER\", \"END_DATE\")\ntable2[ , END_DATE := as.Date(END_DATE, format = \"%B %d,%Y\")]\ntable2 = table2[!is.na(TICKER)]\n\nsp500DT = merge(table1,table2,by=\"TICKER\",all=TRUE)\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05       <NA>\n2:     AA       <NA> 2016-11-01\n\n\n\n\n\ntable1 = table1[[\"Symbol\", \"Date first added\"]].copy()\ntable1.columns = [\"TICKER\", \"START_DATE\"]\ntable1[\"START_DATE\"] = pd.to_datetime(table1[\"START_DATE\"], errors = 'coerce')\ntable1 = table1[table1[\"TICKER\"].notna()]\n\ntable2 = table2[[\"Removed_Ticker\", \"Date_Date\"]].copy()\ntable2.columns = [\"TICKER\", \"END_DATE\"]\ntable2[\"END_DATE\"] = pd.to_datetime(table2[\"END_DATE\"], errors = 'coerce', format = \"%B %d, %Y\")\ntable2 = table2[table2[\"TICKER\"].notna()]\n\n\nsp500DT = pd.merge(table1, table2, on = \"TICKER\", how = 'outer')\nsp500DT = sp500DT.sort_values(by = 'TICKER')\nsp500DT.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05        NaT\n618     AA        NaT 2016-11-01\n\n\n\n\n\nJust by looking at the both initial rows it becomes obvious that we have instances where END_DATE is not available – we can assume this means that those companies are still constituents of the index as of today; we will populate these with a future time to avoid any issues when using the table (e.g. “2111-11-11”). We also have many instances where START_DATE is missing – similarly, we can assume here that these companies have been in the index since the beginning; we can populate this with any date prior to the dates we are interested in (we will just pick the oldest date available in the table).\nUnfortunately, there are few companies with different END_DATEs for which we are missing some information: these are companies that have been incorporated and removed from the index several times, for which we don’t have any of the corresponding START_DATEs. Because there are very few cases and we don’t intend to have very accurate results anyway, we will simply take the last END_DATE and assume these companies were inside the index all the time up until then. To do that we simply get the row with maximum END_DATE per TICKER.\n\nRPython\n\n\n\nsp500DT[is.na(END_DATE), END_DATE := as.Date(\"2111-11-11\")]\nsp500DT[is.na(START_DATE), START_DATE :=min(sp500DT$START_DATE, na.rm = TRUE)]\nsp500DT = sp500DT[ , .SD[which.max(END_DATE)], by = TICKER]\nsp500DT[1:2]\n\n   TICKER START_DATE   END_DATE\n1:      A 2000-06-05 2111-11-11\n2:     AA 1957-03-04 2016-11-01\n\n\n\n\n\nsp500DT[\"END_DATE\"] = sp500DT.END_DATE.fillna(pd.to_datetime(\"2111-11-11\"))\nsp500DT[\"START_DATE\"] = sp500DT.START_DATE.fillna(sp500DT.START_DATE.min())\nsp500DT = sp500DT.sort_values('END_DATE').groupby('TICKER').tail(1).sort_values(by = 'TICKER')\nsp500DT.iloc[:2]\n\n    TICKER START_DATE   END_DATE\n13       A 2000-06-05 2111-11-11\n618     AA 1957-03-04 2016-11-01"
  },
  {
    "objectID": "posts/fetching-data-2/index.html#get-stock-market-data",
    "href": "posts/fetching-data-2/index.html#get-stock-market-data",
    "title": "Fetching & Preparing Data: stock market data for S&P 500 2",
    "section": "2. Get stock market data",
    "text": "2. Get stock market data\nNow that we have a point-in-time version of the S&P 500 index, with all its constituents over time, we can move to retrieving the market data."
  },
  {
    "objectID": "posts/fetching-data-1/index.html#clean-and-prepare-the-data-for-later-use",
    "href": "posts/fetching-data-1/index.html#clean-and-prepare-the-data-for-later-use",
    "title": "Fetching & Preparing Data",
    "section": "3. Clean and prepare the data for later use",
    "text": "3. Clean and prepare the data for later use\nWe are missing 102 from 775 companies. All of them are companies that have been removed from the index, so missing them could introduce survivorship bias. At least few of them are companies that have been acquired by another and Yahoo does not seem to track them in a point-in-time fashion, so the old tickers and corresponding market information have been lost. We will not spend time looking into this, we just want a reasonable sample of market data for illustration purposes.\nAs final steps, we:\n\ntransform the data to the correct format.\nremove rows with missing information (e.g. dates that were returned with NULLs).\nmeasure daily close to close returns. It is important to use Adjusted Close Prices (ADJ.CLOSE) for this, as they incorporate adjustments for splits and dividends.\nremove some of the bad data. There are plenty of issues (e.g. wrong prices) in the Wikipedia market data, we would not recommend to use it for serious research. We are not trying to solve any of those issues here so we will simply remove any daily returns larger than 1 (100% increase) or smaller than -0.8 (80% drop).\nmake the market data time sensitive over the index constituents (currently we have information for the full period of time, if available, for all companies, regardless of when they entered or left the index).\nsave the information in a csv file.\n\n\nRPython\n\n\n\nmarketDT[ , DATE := as.Date(DATE)]\nnamesCol = names(marketDT)\nnamesCol = namesCol[!namesCol %in% c(\"DATE\", \"TICKER\")]\nmarketDT[ , (namesCol) := lapply(.SD, as.numeric), .SDcols = namesCol]\nmarketDT = marketDT[complete.cases(marketDT)]\n\n# measure daily returns\nmarketDT[, ADJ.CLOSE_o := shift(ADJ.CLOSE, 1, type = \"lag\"), by = TICKER]\nmarketDT[, RET:=(ADJ.CLOSE - ADJ.CLOSE_o) / ADJ.CLOSE_o, by = TICKER]\nmarketDT[, ADJ.CLOSE_o := NULL]\n\n# \"remove\" some bad data by converting to NA de daily returns\nmarketDT[RET > 1, RET := NA]\nmarketDT[RET < (-0.8), RET := NA]\n\n# add start and end dates and filter for a point-in-time index\nmarketDT = merge(marketDT, sp500DT, by = \"TICKER\")\nmarketDT = marketDT[DATE >= START_DATE & DATE <= END_DATE]\n\n\n# save data\ndirPath = \"YOUR_DIRECTORY_PATH\" # Put here your path\nfwrite(marketDT, paste0(dirPath, \"marketDT.csv\"))\n\n\n\n\nmarketDF['DATE'] = pd.to_datetime(marketDF['DATE'])\nmarketDF = marketDF.dropna()\n\n# measure daily returns\nmarketDF['ADJ CLOSE o'] = marketDF.groupby('TICKER')['ADJ CLOSE'].shift(1)\nmarketDF['RET'] = (marketDF['ADJ CLOSE'] - marketDF['ADJ CLOSE o'])/marketDF['ADJ CLOSE o']\nmarketDF.drop(columns = 'ADJ CLOSE o', inplace = True)\n\n# \"remove\" some bad data by converting to NA de daily returns\nmarketDF.loc[marketDF.RET > 1, 'RET'] = pd.NA\nmarketDF.loc[marketDF.RET < (-0.8), 'RET'] = pd.NA\n\n# add start and end dates and filter for a point-in-time index\nmarketDF = pd.merge(marketDF, sp500DF, how = \"left\", on = ['TICKER'])\nmarketDF = marketDF[(marketDF.DATE >= marketDF.START_DATE) & (marketDF.DATE <= marketDF.END_DATE)]\n\n\n# save data\ndirPath = \"YOUR_DIRECTORY_PATH\" # Put here your path\nmarketDF.to_csv(dirPath + \"marketDF.csv\" , index=False)\n\n\n\n\nAs a last checkup, we look into the number of companies in the index for which we have pricing information in the table:\n\nRPython\n\n\n\nlibrary(ggplot2)\nNDT = marketDT[!is.na(RET)][, .N, by = DATE]\nggplot(NDT) + geom_line(aes(x = DATE, y= N )) + theme_bw()\n\n\n\n\n\n\n\nimport seaborn as sns\nNDF = marketDF[~marketDF.RET.isna()].groupby('DATE').size().reset_index(name = 'N')\nsns.lineplot(x = 'DATE', y = 'N', data = NDF)\n\n\n\n\n\n\n\nUnfortunately, the numbers are not looking very good; there is a clear deterioration of the index when moving backwards in time. This clearly hints towards survivorship bias, which could affect all our results and jeopardize any of the conclusions. As already mentioned above, we are only going to use this data as a sample, but we still wanted to point out the potential issues when using low quality data."
  }
]